{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8592ad18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "DATA_DIR = Path(\"~/data/TREC\").expanduser()\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE_IDS = list(range(torch.cuda.device_count()))\n",
    "print(f\"Devices: {DEVICE_IDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57577c",
   "metadata": {},
   "source": [
    "Load the **TREC** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd05bbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset trec (/home/IAIS/hiser/.cache/huggingface/datasets/trec/default/2.0.0/f2469cab1b5fceec7249fda55360dfdbd92a7a5b545e91ea0f78ad108ffac1c2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e8ccf2b0724591bd7b8f524a994256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'coarse_label', 'fine_label'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trec\")\n",
    "dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f446d3",
   "metadata": {},
   "source": [
    "Load Finetuned **ColBERT** model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a1a42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 10:04:11.685051: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-13 10:04:13.428217: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-13 10:04:22.329692: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-13 10:04:22.329916: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-13 10:04:22.329924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (helpers.py, line 4)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/explain/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3442\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 2\u001b[0;36m\n\u001b[0;31m    from helpers import ColBERT\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/dev/explainable-transformers/helpers.py:4\u001b[0;36m\u001b[0m\n\u001b[0;31m    rom transformers import AutoModel, AutoTokenizer, PretrainedConfig, PreTrainedModel\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from helpers import ColBERT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = ColBERT.from_pretrained(\"sebastian-hofstaetter/colberter-128-32-msmarco\")\n",
    "net = torch.nn.DataParallel(model.cuda(DEVICE_IDS[0]), DEVICE_IDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da60a1",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## 2-Step evaluation (using `faiss`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b312775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def encode_dataset(dataset, model, tokenizer):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=512)\n",
    "    \n",
    "    index, embeddings = [], []\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        input_dict = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            embedding = model(**input_dict).cpu().numpy()\n",
    "        \n",
    "        attention = input_dict[\"attention_mask\"].bool()\n",
    "        embedding = embedding[attention]\n",
    "        indices = np.where(attention)[0] + dataloader.batch_size * i\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "        index.append(indices)\n",
    "    \n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    index = np.concatenate(index)\n",
    "    \n",
    "    return embeddings, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ef397",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embed, train_index = encode_dataset(dataset[\"train\"], net, tokenizer)\n",
    "test_embed, test_index = encode_dataset(dataset[\"test\"], net, tokenizer)\n",
    "print(train_embed.shape)\n",
    "print(test_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import time\n",
    "\n",
    "d = train_embed.shape[-1]\n",
    "n_centroids = 1_000\n",
    "code_size = 16\n",
    "n_bits = 8\n",
    "\n",
    "coarse_quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFPQ(coarse_quantizer, d, n_centroids, code_size, 8)\n",
    "index.nprobe = 10\n",
    "\n",
    "start = time.time()\n",
    "index.train(train_embed)\n",
    "index.add(train_embed)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time {end - start:.2f}sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46506098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers as h\n",
    "\n",
    "import importlib\n",
    "importlib.reload(h)\n",
    "\n",
    "def candidate_generation(I: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    candidates = np.unique(train_index[I])\n",
    "    candidate_index = train_index[np.isin(train_index, candidates)]\n",
    "    candidate_embed = train_embed[candidate_index]\n",
    "\n",
    "    return h.flattened_to_batched(train_embed[candidate_index], candidate_index, return_att_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "_, I = index.search(test_embed, k=50)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time {end - start:.2f}sec\")\n",
    "\n",
    "document_vecs, document_mask = candidate_generation(I)\n",
    "query_vecs, query_mask = h.flattened_to_batched(test_embed, test_index, padding=document_vecs.shape[1], return_att_mask=True)\n",
    "# convert to torch tensors\n",
    "document_vecs, document_mask = torch.from_numpy(document_vecs), torch.from_numpy(document_mask)\n",
    "query_vecs, query_mask = torch.from_numpy(query_vecs), torch.from_numpy(query_mask)\n",
    "\n",
    "print(document_vecs.shape)\n",
    "print(query_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array that holds all possible index pair combinations\n",
    "query_ind = np.arange(query_vecs.shape[0])\n",
    "doc_ind = np.arange(document_vecs.shape[0])\n",
    "index_pairs = np.array(np.meshgrid(query_ind, doc_ind)).T.reshape(-1, 2)\n",
    "print(f\"#pairs: {index_pairs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feebe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.forward_aggregation(\n",
    "    query_vecs[index_pairs[:, 0]],\n",
    "    document_vecs[index_pairs[:, 1]],\n",
    "    query_mask[index_pairs[:, 0]],\n",
    "    document_mask[index_pairs[:, 1]]\n",
    ")\n",
    "scores = scores.reshape(query_vecs.shape[0], document_vecs.shape[0])\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a38ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, i = scores.max(1)\n",
    "i = np.unique(train_index[I])[i]\n",
    "\n",
    "labels = np.array(dataset[\"train\"][\"coarse_label\"])\n",
    "y_pred = labels[real_i]\n",
    "print(y_pred.shape)\n",
    "\n",
    "y_true = np.array(dataset[\"test\"][\"coarse_label\"])\n",
    "print(y_true.shape)\n",
    "\n",
    "(y_pred == y_true).sum() / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ee9411",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Exhaustive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def encode_dataset_2(dataset, model, tokenizer):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=512)\n",
    "    \n",
    "    attention, embeddings = [], []\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        input_dict = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            embedding = model(**input_dict).cpu().numpy()\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "        attention.append(input_dict[\"attention_mask\"].bool())\n",
    "    \n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    attention = np.concatenate(attention)\n",
    "    \n",
    "    return embeddings, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707ae36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_embed, train_att = encode_dataset_2(dataset[\"train\"], net, tokenizer)\n",
    "test_embed, test_att = encode_dataset_2(dataset[\"test\"], net, tokenizer)\n",
    "print(train_embed.shape)\n",
    "print(test_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensors\n",
    "document_vecs, document_mask = torch.from_numpy(train_embed).float(), torch.from_numpy(train_att).float()\n",
    "query_vecs, query_mask = torch.from_numpy(test_embed).float(), torch.from_numpy(test_att).float()\n",
    "\n",
    "document_vecs = document_vecs[:, :50]\n",
    "query_vecs = query_vecs[:, :50]\n",
    "document_mask = document_mask[:, :50]\n",
    "query_mask = query_mask[:, :50]\n",
    "\n",
    "print(document_vecs.shape)\n",
    "print(query_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array that holds all possible index pair combinations\n",
    "query_ind = np.arange(query_vecs.shape[0])\n",
    "doc_ind = np.arange(document_vecs.shape[0])\n",
    "index_pairs = np.array(np.meshgrid(query_ind, doc_ind)).T.reshape(-1, 2)\n",
    "print(f\"#pairs: {index_pairs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ac153",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.forward_aggregation(\n",
    "    query_vecs[index_pairs[:, 0]],\n",
    "    document_vecs[index_pairs[:, 1]],\n",
    "    query_mask[index_pairs[:, 0]],\n",
    "    document_mask[index_pairs[:, 1]]\n",
    ")\n",
    "scores = scores.reshape(test_embed.shape[0], train_embed.shape[0])\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a622934",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, i = scores.max(1)\n",
    "\n",
    "labels = np.array(dataset[\"train\"][\"coarse_label\"])\n",
    "y_pred = labels[real_i]\n",
    "print(y_pred.shape)\n",
    "\n",
    "y_true = np.array(dataset[\"test\"][\"coarse_label\"])\n",
    "print(y_true.shape)\n",
    "\n",
    "(y_pred == y_true).sum() / y_true.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
