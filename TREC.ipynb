{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36717b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "DATA_DIR = Path(\"~/data/TREC\").expanduser()\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE_IDS = list(range(torch.cuda.device_count()))\n",
    "print(f\"Devices: {DEVICE_IDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9d87c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset trec (/home/IAIS/hiser/.cache/huggingface/datasets/trec/default/2.0.0/f2469cab1b5fceec7249fda55360dfdbd92a7a5b545e91ea0f78ad108ffac1c2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2817ab861a884f51ab04fad32216ed30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"trec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e399d927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'coarse_label', 'fine_label'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d88cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def encode_dataset(dataset, model, tokenizer):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=512)\n",
    "    \n",
    "    index, embeddings = [], []\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        input_dict = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            embedding = model(**input_dict).cpu().numpy()\n",
    "        \n",
    "        attention = input_dict[\"attention_mask\"].bool()\n",
    "        embedding = embedding[attention]\n",
    "        indices = np.where(attention)[0] + dataloader.batch_size * i\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "        index.append(indices)\n",
    "    \n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    index = np.concatenate(index)\n",
    "    \n",
    "    return embeddings, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0bb2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 17:15:10.654535: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 17:15:10.812368: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-12 17:15:11.700907: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-12 17:15:11.700996: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-12 17:15:11.701003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at sebastian-hofstaetter/uni-colberter-128-1-msmarco were not used when initializing ColBERT: ['stop_word_reducer.weight', 'score_merger', 'compressor_retrieval.bias', 'compressor_retrieval.weight', 'stop_word_reducer.bias', 'mini_compressor.weight', 'mini_compressor.bias']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from helpers import ColBERT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = ColBERT.from_pretrained(\"sebastian-hofstaetter/colberter-128-32-msmarco\")\n",
    "net = torch.nn.DataParallel(model.cuda(DEVICE_IDS[0]), DEVICE_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a727a55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90b6903a95b4e26b65d07f79d550cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b958aae01c0f447d8cd1d679e24b844f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73442, 32)\n",
      "(5162, 32)\n"
     ]
    }
   ],
   "source": [
    "train_embed, train_index = encode_dataset(dataset[\"train\"], net, tokenizer)\n",
    "test_embed, test_index = encode_dataset(dataset[\"test\"], net, tokenizer)\n",
    "print(train_embed.shape)\n",
    "print(test_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838523a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 8.02sec\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import time\n",
    "\n",
    "d = train_embed.shape[-1]\n",
    "n_centroids = 1_000\n",
    "code_size = 16\n",
    "n_bits = 8\n",
    "\n",
    "coarse_quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFPQ(coarse_quantizer, d, n_centroids, code_size, 8)\n",
    "index.nprobe = 10\n",
    "\n",
    "start = time.time()\n",
    "index.train(train_embed)\n",
    "index.add(train_embed)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time {end - start:.2f}sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0854c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers as h\n",
    "\n",
    "import importlib\n",
    "importlib.reload(h)\n",
    "\n",
    "def candidate_generation(I: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    candidates = np.unique(train_index[I])\n",
    "    candidate_index = train_index[np.isin(train_index, candidates)]\n",
    "    candidate_embed = train_embed[candidate_index]\n",
    "\n",
    "    return h.flattened_to_batched(train_embed[candidate_index], candidate_index, return_att_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e48e01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.02sec\n",
      "torch.Size([5452, 41, 32])\n",
      "torch.Size([500, 41, 32])\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "_, I = index.search(test_embed, k=50)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time {end - start:.2f}sec\")\n",
    "\n",
    "document_vecs, document_mask = candidate_generation(I)\n",
    "query_vecs, query_mask = h.flattened_to_batched(test_embed, test_index, padding=document_vecs.shape[1], return_att_mask=True)\n",
    "# convert to torch tensors\n",
    "document_vecs, document_mask = torch.from_numpy(document_vecs), torch.from_numpy(document_mask)\n",
    "query_vecs, query_mask = torch.from_numpy(query_vecs), torch.from_numpy(query_mask)\n",
    "\n",
    "print(document_vecs.shape)\n",
    "print(query_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d932150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#pairs: 2726000\n"
     ]
    }
   ],
   "source": [
    "# create array that holds all possible index pair combinations\n",
    "query_ind = np.arange(query_vecs.shape[0])\n",
    "doc_ind = np.arange(document_vecs.shape[0])\n",
    "index_pairs = np.array(np.meshgrid(query_ind, doc_ind)).T.reshape(-1, 2)\n",
    "print(f\"#pairs: {index_pairs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01df0284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 5452])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.forward_aggregation(\n",
    "    query_vecs[index_pairs[:, 0]],\n",
    "    document_vecs[index_pairs[:, 1]],\n",
    "    query_mask[index_pairs[:, 0]],\n",
    "    document_mask[index_pairs[:, 1]]\n",
    ")\n",
    "scores = scores.reshape(query_vecs.shape[0], document_vecs.shape[0])\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a277ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, i = scores.max(1)\n",
    "# real_i = np.unique(train_index[I])[i]\n",
    "real_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "182fb570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(dataset[\"train\"][\"coarse_label\"])\n",
    "y_pred = labels[real_i]\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbdaee1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array(dataset[\"test\"][\"coarse_label\"])\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a48a7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.188"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred == y_true).sum() / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3255f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853339f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d0069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
